{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import wandb\n",
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import src.const as const\n",
    "import src.env.env_basic as env_basic\n",
    "import src.agents.agent_greedy as agent_greedy\n",
    "import src.agents.agent_nerdy as agent_nerdy\n",
    "import src.agents.agent_ppo as agent_ppo\n",
    "import src.simulation.water_demands as wd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent nerdy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 242\n",
    "num_leaks = 12\n",
    "df = wd.load(seed, num_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': seed,\n",
    "    'num_leaks': num_leaks,\n",
    "    'env': 'EnvComplexR',\n",
    "}\n",
    "# test on last 30 days\n",
    "env = env_basic.EnvComplexR(df, train=False, obs_len=10)\n",
    "\n",
    "agent = agent_nerdy.AgentAlways()\n",
    "run = wandb.init(project= 'water_demand_rl', config=args, monitor_gym=True)\n",
    "run.name = f\"agent_always_{run.id}\"\n",
    "_obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "sum_rewards = 0\n",
    "while not done:\n",
    "    action = agent.take_action(_obs)\n",
    "    _obs, reward, done, info, _ = env.step(action)\n",
    "\n",
    "    # log to wandb\n",
    "    sum_rewards += reward\n",
    "    run.log({'test_action': action, \"test_step_reward\": reward, 'test_sum_reward': sum_rewards})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': seed,\n",
    "    'num_leaks': num_leaks,\n",
    "    'env': 'EnvComplexR',\n",
    "}\n",
    "# test on last 30 days\n",
    "env = env_basic.EnvComplexR(df, train=False, obs_len=10)\n",
    "agent = agent_nerdy.AgentNever()\n",
    "run = wandb.init(project= 'water_demand_rl', config=args, monitor_gym=True)\n",
    "run.name = f\"agent_never_{run.id}\"\n",
    "_obs = env.reset()\n",
    "done = False\n",
    "\n",
    "sum_rewards = 0\n",
    "while not done:\n",
    "    action = agent.take_action(_obs)\n",
    "    _obs, reward, done, info, _ = env.step(action)\n",
    "\n",
    "    # log to wandb\n",
    "    sum_rewards += reward\n",
    "    run.log({'test_action': action, \"test_step_reward\": reward, 'test_sum_reward': sum_rewards})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 242\n",
    "num_leaks = 12\n",
    "df = wd.load(seed, num_leaks)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': seed,\n",
    "    'num_leaks': num_leaks,\n",
    "    'eps': 0,\n",
    "    'epochs': epochs,\n",
    "    'Env': 'EnvComplexR',\n",
    "}\n",
    "\n",
    "\n",
    "for eps in [0.01, 0.1, 0.2]:\n",
    "    args['eps'] = eps\n",
    "    run = wandb.init(project= 'water_demand_rl', config=args, monitor_gym=True)\n",
    "    run.name = f\"agent_greedy_{run.id}\"\n",
    "\n",
    "    env = env_basic.EnvComplexR(df, train=True, obs_len=10)\n",
    "    agent = agent_greedy.AgentGreedy()\n",
    "    # train \n",
    "    episode_return = 0\n",
    "    for epiosed in range(epochs):\n",
    "        _obs,info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.take_action(eps=eps)\n",
    "            _obs, reward, done, info, _  = env.step(action)\n",
    "            agent.update(action, reward)\n",
    "\n",
    "            # log to wandb\n",
    "            episode_return += reward\n",
    "        run.log({'epiosed': epiosed, 'epiosed_sum_reward': episode_return})\n",
    "\n",
    "    # memory dump\n",
    "    data_path = f'data/train/eps_greedy_record_{run.id}.npy'\n",
    "    agent.dump_record(data_path)\n",
    "\n",
    "    # test on last 30 days\n",
    "    env.train = False\n",
    "    agent.load_record(data_path) # load memory\n",
    "\n",
    "    _obs,info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    sum_rewards = 0\n",
    "    while not done:\n",
    "        action = agent.take_action(eps=eps)\n",
    "        _obs, reward, done, info,_  = env.step(action)\n",
    "        \n",
    "        # log to wandb\n",
    "        sum_rewards += reward\n",
    "        run.log({'test_action': action, \"test_step_reward\": reward, 'test_sum_reward': sum_rewards})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(path = \"adamzh0u/water_demand_rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get records from wandb\n",
    "dic_test_sum_rewards = {}\n",
    "\n",
    "for run in runs:\n",
    "    name = run.name\n",
    "    config = run.config\n",
    "    print(name, config)\n",
    "    history = run.scan_history(keys=[\"test_sum_reward\"])\n",
    "    test_sum_rewards = [row['test_sum_reward'] for row in history]\n",
    "    dic_test_sum_rewards[name] = test_sum_rewards\n",
    "\n",
    "dic_train_sum_rewards = {}\n",
    "for run in runs:\n",
    "    name = run.name\n",
    "    history = run.scan_history(keys=[\"epiosed_sum_reward\"])\n",
    "    train_sum_rewards = [row['epiosed_sum_reward'] for row in history]\n",
    "    dic_train_sum_rewards[name] = train_sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_labels1 = { \n",
    "                'agent_greedy_w6vkzwta': 'Greedy $\\epsilon = 0.01$',\n",
    "                'agent_greedy_3trkrt17': 'Greedy $\\epsilon = 0.1$',\n",
    "                'agent_greedy_fv9ax2yk': 'Greedy $\\epsilon = 0.2$',\n",
    "                'agent_always_wt7zj6fu':'Always',\n",
    "                'agent_never_rch83t0r':'Never',}\n",
    "dic_labels2 = { \n",
    "                'agent_dqn_jtsyim1g': 'DQN $\\gamma = 0.5$',\n",
    "                'agent_dqn_is8m298o':'DQN $\\gamma = 0.2$',\n",
    "                'agent_dqn_fn67te1w':'DQN $\\gamma = 0.7$',\n",
    "                # 'agent_greedy_w6vkzwta': 'Greedy $\\epsilon = 0.01$',\n",
    "                # 'agent_always_fn67te1w':'Always',\n",
    "                # 'agent_never_rch83t0r':'Never',\n",
    "                }\n",
    "dic_labels3= { 'agent_dqn_sxuseuwu':'DQN $epoch=500$',\n",
    "                'agent_greedy_w6vkzwta': 'Greedy $\\epsilon = 0.01$',\n",
    "                # 'agent_greedy_3trkrt17': 'Greedy $\\epsilon = 0.1$',\n",
    "                # 'agent_greedy_fv9ax2yk': 'Greedy $\\epsilon = 0.2$',\n",
    "                # 'agent_always_fn67te1w':'Always',\n",
    "                'agent_never_rch83t0r':'Never',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.plot_utils as pu\n",
    "import matplotlib.pyplot as plt\n",
    "pu.setup_mpl(as_default=0)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,2, figsize=(3.60236*2, 7),dpi=300)\n",
    "ax = ax.flatten()\n",
    "\n",
    "train_sum_rewards = dic_train_sum_rewards['agent_dqn_sxuseuwu']\n",
    "smooth_record = np.convolve(train_sum_rewards, np.ones(10)/10, mode='valid')\n",
    "ax[0].plot(smooth_record)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Train sum reward for each epoch')\n",
    "\n",
    "for name in dic_labels1.keys():\n",
    "    record = [0] + dic_test_sum_rewards[name]\n",
    "    ax[1].plot(record, label=dic_labels1[name])\n",
    "ax[1].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Test sum reward')\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "for name in dic_labels2.keys():\n",
    "    record = [0] + dic_test_sum_rewards[name]\n",
    "    ax[2].plot(record, label=dic_labels2[name])\n",
    "ax[2].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Test sum reward')\n",
    "ax[2].legend(frameon=False)\n",
    "\n",
    "\n",
    "for name in dic_labels3.keys():\n",
    "    record = [0] + dic_test_sum_rewards[name]\n",
    "    ax[3].plot(record, label=dic_labels3[name])\n",
    "ax[3].set_xlabel('Steps')\n",
    "ax[3].legend(frameon=False)\n",
    "\n",
    "for i in range(4):\n",
    "    # label A B \n",
    "    ax[i].text(-0.1, 1.01, chr(65+i), transform=ax[i].transAxes,\n",
    "            size=12, weight='bold')\n",
    "fig.savefig('fig/fig_results.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
